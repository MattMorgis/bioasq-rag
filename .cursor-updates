# .cursor-updates

- Implemented BioPythonPubMedClient using Biopython for PubMed abstract retrieval with tests
- Created DataFetcher class to fetch PubMed abstracts from a hardcoded URL with tests
- Added a run_fetcher.py script to demonstrate how to use the DataFetcher
- Moved PubMed client files into a separate clients subdirectory to improve organization
- Created PubMedURLCollector to extract and deduplicate all PubMed URLs from BioASQ datasets
- Enhanced DataFetcher to use PubMedURLCollector for parallel batch downloading of 50,000+ abstracts with rate limiting and retry logic
- Improved DataFetcher to process multiple batches concurrently while each batch processes URLs sequentially for better parallel request control and rate limiting
- Added support for loading NCBI API key from .env file automatically when running data_pipelines
- Removed batch PubMed API requests in BioPythonPubMedClient in favor of individual requests to address API errors and improve reliability
- Optimized DataFetcher to utilize full API rate limit of 10 requests per second, replacing batch sequential processing with fully concurrent request model
- Improved HTTP 429 rate limit error handling with specific PubMedRateLimitError exception and exponential backoff retry strategy
- Added functionality to track and save failed download URLs to data/failed_urls.json for later retry
- Added centralized logging utility for consistent logging to both console and file across all modules with configurable log levels
